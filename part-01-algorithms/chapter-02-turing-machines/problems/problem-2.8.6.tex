\subsection*{Problem 2.8.6}
\addcontentsline{toc}{subsection}{Problem 2.8.6}

\paragraph{Statement.}
Give another proof of Theorem 2.1 by presenting the \(k\) strings not next to each other, but on top of each other.
Analyze the efficiency of your simulation and compare with the proof of Theorem 2.1.
(A single symbol of the simulating machine will encode \(k\) symbols of the simulated one. Naturally, the cursors will be all over the place.)

\paragraph{Solution.}
Let \(M\) be a \gls{multitapetm} with \gls{tapealphabet} \(\Gamma\), and suppose \(M\) runs for at most \(t(n)\) steps
on inputs of length \(n\). We construct a \gls{singletapetm} \(M'\) that simulates \(M\) by encoding the \(k\) tapes
\emph{on top of each other} (multi-\gls{track} encoding). This is a standard simulation trick; see, e.g.,
\cite{sipser2012introduction,hopcroftullman1979} for related encodings and simulations.

\paragraph{Encoding (stacked tracks).}
At each tape position \(i\ge 0\), \(M'\) stores a \(k\)-tuple \((a_1,\dots,a_k)\in \Gamma^k\), where \(a_j\) is the symbol
in cell \(i\) of tape \(j\) of \(M\). Thus \emph{one} symbol of \(M'\) encodes \(k\) symbols of \(M\), exactly as required.

To represent head positions, each track uses a marked version of \(\Gamma\): for each \(a\in\Gamma\) there is a marked
symbol \(\widehat{a}\). A \gls{configuration} is encoded so that, in each track \(j\), exactly one cell is marked
(the cell currently scanned by tape \(j\)'s head). The alphabet of \(M'\) is therefore a constant-size expansion of \(\Gamma^k\),
since \(k\) is fixed.

\paragraph{Simulating one step.}
Assume \(M\) is in some configuration \(C\). To simulate one transition of \(M\), \(M'\) must:
\begin{enumerate}[label=(\roman*)]
  \item locate, on each track, the unique marked symbol (these are the \(k\) scanned symbols of \(M\)),
  \item apply \(M\)'s transition function in its finite control (possible because \(k\) is constant),
  \item update the \(k\) written symbols and move each head marker left/right/stay accordingly.
\end{enumerate}

Because the \(k\) head positions may be far apart, \(M'\) finds them by sweeping over the \emph{active} portion of its tape.
Let \(L\) be the maximum tape index ever visited by any head of \(M\) up to the current time.
Then the entire configuration \(C\) is represented within cells \(0,1,\dots,L\) of \(M'\)'s tape.

A single simulated step can be done with a constant number of full sweeps over \(0..L\):
one sweep to read the \(k\) marked symbols, and one sweep to perform the corresponding updates and move the markers.
Therefore one step of \(M\) costs \(O(L)\) steps of \(M'\).

\paragraph{Running time bound.}
After \(r\) simulated steps, each head of \(M\) has moved at most \(r\) times, hence \(L \le O(n+r)\).
In particular, throughout the simulation of a \(t(n)\)-step computation, we have \(L=O(n+t(n))=O(t(n))\).
Thus \(M'\) uses \(O(t(n))\) time to simulate each step of \(M\), and the total simulation time is
\[
O\!\left(\sum_{r=1}^{t(n)} L_r\right) \;=\; O\!\left(\sum_{r=1}^{t(n)} (n+r)\right) \;=\; O\!\left(t(n)^2 + n\,t(n)\right)
\;=\; O\!\left(t(n)^2\right).
\]

\paragraph{Comparison with Theorem 2.1.}
Theorem 2.1 shows that multitape machines can be simulated by a single-tape machine with at most quadratic slowdown.
The stacked-track encoding yields the same asymptotic overhead as the standard ``strings side by side'' proof:
the key cost is that, with one head, \(M'\) must sweep over the active region to coordinate \(k\) independent head positions.
The difference is mainly representational (a constant-factor change in alphabet size and bookkeeping), not asymptotic.
